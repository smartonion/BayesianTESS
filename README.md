# BayesianTESS
Applying Bayesian Methods in Machine Learning on TESS Light Curves

## What's been done

Added the TOI dataset from NASA Exoplanet Archive. Created a processing script that converts the raw TOI data into a clean format with one row per star. The script filters out ambiguous cases and dispositions we don't want (PC, APC, KP), and creates binary labels: 1 for confirmed planets, 0 for false positives/alarms. Output is saved to `dataset/toi_per_star_labels.csv` with columns: tic_id, label, period_days, t0_bjd. This is the file that'll be used for the GP training pipeline.

Built a complete data pipeline for downloading and processing TESS light curves. Created scripts to download SPOC light curve FITS files from MAST, convert them to compressed NPZ format, and handle cases where stars don't have SPOC data available. The main script is `scripts/build_dataset.py` which does everything automatically - creates a subset (default: 60 CP, 90 FP/FA stars) or uses the full dataset, downloads their light curves, converts to NPZ, and automatically backfills any stars that don't have data available. The script has resume logic so it can be interrupted and restarted without re-downloading or re-converting existing files. It now supports command-line arguments: `--use_full` to use the entire dataset (1901 stars), `--target_cp` and `--target_fp` for custom subset sizes, and `--dry_run` for testing without downloading. Final output is `dataset/index.csv` which defines the training data contract with columns: tic_id, npz_path, label, period_days, t0_bjd. Each NPZ file contains time, flux, flux error, quality flags, and metadata arrays sorted by time with NaN values filtered out. The dataset is ready for GP training with the configured number of stars all having valid light curve data.

Created a class-based DataLoader in `models/data_loader.py` that aggregates light curve data into lists for ML training. The DataLoader reads from `index.csv`, loads all NPZ files, and returns (X, y) where X is a list of flux arrays and y is a numpy array of binary labels. It reuses existing functionality from `gp_data.py` and handles path resolution automatically.

Implemented feature extraction for baseline GP model in `models/features.py`. Extracts 6 features from light curves: mean, std, skewness, kurtosis, dominant period, and peak power. Features are computed on normalized flux (median normalization) and include both statistical and frequency domain analysis. Created `scripts/build_feature_matrix.py` that processes all stars from the training index, extracts features, and saves the feature matrix (X) and labels (y) to disk as NumPy arrays for baseline GP model training.

Implemented baseline GP classifier training in `scripts/train_baseline_gp.py` using GPyTorch. The model uses variational inference with RBF kernel for binary classification. Training uses k-fold cross-validation (5 folds by default) with stratified splitting to ensure balanced class distribution. Each training run creates a date-based folder (`saved_models/baseline_gp/YYYYMMDD_HHMMSS/`) containing all fold models and the best model checkpoint. Features are normalized for numerical stability, and the model achieves ~60% accuracy on the 150-star dataset. The training script includes TensorBoard logging and loss plots for monitoring.

Implemented GAK GP classifier training in `scripts/train_gak_gp.py` using precomputed GAK kernel matrices. The model uses variational inference with a custom precomputed kernel wrapper. Training includes validation monitoring, TensorBoard logging, and saves loss/accuracy plots. The model achieves competitive performance with fast training times (~2.4 minutes for 5-fold CV on 150 samples).

Implemented high-resolution SoftDTW GP classifier training in `scripts/train_gp_highres.py` using the StableSoftDTWKernelV3. The model handles long sequences (L=1024+) with tiled computation and bandwidth pruning for memory efficiency. Training includes validation monitoring, TensorBoard logging, validation curve plots with closest inducing points, and comprehensive model checkpoints.